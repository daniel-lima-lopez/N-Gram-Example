{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Gram matriz count\n",
    "This Notebook identifies unique words in Shakespeare's works. In addition, a count of all the bigrams present in the text corpus is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to tokenize the input sentences. In each case, symbols are removed, the sentence is converted to lowercase, and multiple spaces are removed. Finally, the processed text is returned as a list of words, including a special token for the star (s1) and end (e1) of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, n):\n",
    "    # eliminate symbols on input text\n",
    "    sims = \"!\\\"#$%&()*+-.,'/:;<=>?@[\\]^_`{|}~\\n\\t\"\n",
    "    for si in sims:\n",
    "        text = text.replace(si, '')\n",
    "\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "\n",
    "    # replace multiple spaces by single\n",
    "    _RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "    text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "    # insert start and end tokens\n",
    "    split = text.split(' ')\n",
    "    st = [f's{i+1}' for i in range(n-1)]\n",
    "    en = [f'e{i+1}' for i in range(n-1)]\n",
    "    split = st + split + en\n",
    "\n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data, which consists of dialogues between characters, is then read. Examples of data evaluations with the `tokenize_text()` function is then displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['So shaken as we are, so wan with care,',\n",
       "       'Find we a time for frighted peace to pant,',\n",
       "       'And breathe short-winded accents of new broils',\n",
       "       'To be commenced in strands afar remote.',\n",
       "       'No more the thirsty entrance of this soil'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/Shakespeare_data.csv')\n",
    "\n",
    "# only valid lines\n",
    "lines = data['PlayerLine'].values\n",
    "indx = data['PlayerLinenumber'].isna().values\n",
    "indx = [not i for i in indx]\n",
    "lines = lines[indx]\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible bigrams are counted, considering the vocabulary available in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with all words with repetition\n",
    "words = []\n",
    "for li in lines:\n",
    "    tokens = tokenize_text(li,n=2)\n",
    "    words += tokens\n",
    "len(words)\n",
    "unique_words, count = np.unique(words, return_counts=True)\n",
    "\n",
    "words = None # free memory\n",
    "\n",
    "# create a word-id dictionary\n",
    "word_id = {}\n",
    "for i, wi in enumerate(unique_words):\n",
    "    word_id[wi] = i\n",
    "\n",
    "# matrix generation\n",
    "Cmatrix = np.zeros(shape=(len(count),len(count)), dtype=np.int32)\n",
    "for li in lines:\n",
    "    tokens = tokenize_text(li, n=2)\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        t1 = tokens[i]\n",
    "        t2 = tokens[i+1]\n",
    "        Cmatrix[word_id[t1],word_id[t2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data necessary to implement the bigram system is written into .csv files. First, a dictionary word-id is written, that identifies each word in the corups with its index in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word-id dictionary\n",
    "ids = [word_id[ki] for ki in word_id.keys()]\n",
    "\n",
    "data_dicc = pd.DataFrame({'id': ids,\n",
    "                          'word': word_id.keys()})\n",
    "data_dicc.to_csv('word_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the bigram count matrix is ​​written. For this, the scarcuty presented is considered, so only the non-zero counts are saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the existing combinations and counts\n",
    "aux_counts = []\n",
    "for row in Cmatrix:\n",
    "    aux = ''\n",
    "    for i, ci in enumerate(row):\n",
    "        if ci > 0:\n",
    "            if aux == '':\n",
    "                aux += f'{i}:{ci}'\n",
    "            else:\n",
    "                aux += f',{i}:{ci}'\n",
    "    aux_counts.append(aux)\n",
    "aux_counts\n",
    "\n",
    "# data from Sparce matrix CMatrix\n",
    "data_sparse = pd.DataFrame({'id': ids,\n",
    "                            'counts': aux_counts})\n",
    "data_sparse.to_csv('CMatrix.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
